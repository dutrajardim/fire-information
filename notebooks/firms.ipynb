{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (expr, broadcast)\n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType, FloatType, TimestampType)\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import (KryoSerializer, SedonaKryoRegistrator)\n",
    "from sedona.utils.adapter import Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf()\n",
    "parser = configparser.ConfigParser()\n",
    "parser.optionxform=str\n",
    "parser.read_file(open('../sparkconf.cfg'))\n",
    "\n",
    "for section, config in parser.items():\n",
    "    for key, value in config.items():\n",
    "        sparkConf.set(key, value)\n",
    "\n",
    "sparkConf.set(\"spark.serializer\", KryoSerializer.getName)\n",
    "sparkConf.set(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "\n",
    "sparkConf.set(\"spark.archives\", \"https://minio.minio-tenant/dutrajardim-etls/dependencies.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=admin%2F20220228%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220228T111424Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=a20e1526dd993354ae90c90c7cb76da4726a5ae00c9920ceec8e2a35f3482c3c#deps\")\n",
    "sparkConf.set(\"spark.executorEnv.PYTHONPATH\", \"/opt/spark/work-dir/deps\")\n",
    "sparkConf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", \"/opt/spark/work-dir/deps/Shapely.libs\")\n",
    "# sparkConf.set(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.38:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://127.0.0.1:16443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Local spark session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8204043cd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SedonaRegistrator.registerAll(spark)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/06 11:02:22 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "22/03/06 11:02:22 WARN AmazonHttpClient: SSL Certificate checking for endpoints has been explicitly disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = \"s3a://dutrajardim-fi\"\n",
    "s3_source = \"%s/src/firms/suomi_viirs_c2/archive/{2021}.csv.gz\" % bucket\n",
    "\n",
    "df_firms = spark.read.format(\"csv\").option(\"header\", \"true\").load(s3_source)\n",
    "\n",
    "sdf_firms = df_firms \\\n",
    "    .selectExpr(\n",
    "        \"ST_GeomFromWKT(CONCAT('POINT(', longitude, ' ', latitude, ')')) as geometry\", \n",
    "        \"brightness\", \n",
    "        \"frp\", \n",
    "        \"scan\", \n",
    "        \"track\", \n",
    "        \"confidence\", \n",
    "        \"type\", \n",
    "        \"instrument\",\n",
    "        \"CONCAT(acq_date, ' ', regexp_replace(acq_time, '(.{2})(.{2})', '$1:$2')) as datetime\",\n",
    "        \"SUBSTRING(acq_date, 1, 4) as year\"\n",
    "    )\n",
    "\n",
    "rdd_firms = Adapter.toSpatialRdd(sdf_firms, \"geometry\")\n",
    "rdd_firms.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_adm3 = spark.read.format(\"parquet\").load(\"s3a://dutrajardim-fi/tables/shapes/adm3.parquet\")\n",
    "df_adm2 = spark.read.format(\"parquet\").load(\"s3a://dutrajardim-fi/tables/shapes/adm2.parquet\")\n",
    "\n",
    "sdf_adm3 = df_adm3.selectExpr(\n",
    "    \"id as adm3\",\n",
    "    \"ST_GeomFromWKT(geometry) as geometry_adm3\",\n",
    "    \"CONCAT(CONCAT_WS('.', SLICE(SPLIT(id, '\\\\\\\\.'), 1, 3)), '_1') as adm2\"\n",
    ")\n",
    "\n",
    "sdf_adm2 = df_adm2.selectExpr(\n",
    "    \"id as adm2\",\n",
    "    \"ST_GeomFromWKT(geometry) as geometry_adm2\"\n",
    ")\n",
    "\n",
    "sdf_adm = broadcast(sdf_adm2).join(sdf_adm3, on='adm2', how='left')\n",
    "sdf_adm = sdf_adm.selectExpr(\n",
    "    \"CASE WHEN geometry_adm3 IS NOT NULL THEN geometry_adm3 ELSE geometry_adm2 END as geometry\",\n",
    "    \"ELEMENT_AT(SPLIT(adm2, '\\\\\\\\.'), 1) as adm0\",\n",
    "    \"CONCAT(CONCAT_WS('.', SLICE(SPLIT(adm2, '\\\\\\\\.'), 1, 2)), '_1') as adm1\",\n",
    "    \"adm2\",\n",
    "    \"adm3\"\n",
    ")\n",
    "\n",
    "rdd_adm = Adapter.toSpatialRdd(sdf_adm, \"geometry\")\n",
    "rdd_adm.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from sedona.core.enums import GridType\n",
    "from sedona.core.enums import IndexType\n",
    "from sedona.core.spatialOperator import JoinQueryRaw\n",
    "\n",
    "rdd_adm.spatialPartitioning(GridType.KDBTREE)\n",
    "rdd_firms.spatialPartitioning(rdd_adm.getPartitioner())\n",
    "\n",
    "# second param is buildIndexOnSpatialPartitionedRDD - set to true as we will run a join query\n",
    "rdd_firms.buildIndex(IndexType.QUADTREE, True)\n",
    "\n",
    "# third param set using index to true while the fourth param set consider boundary intersection to true\n",
    "query_result = JoinQueryRaw.SpatialJoinQueryFlat(rdd_firms, rdd_adm, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_columns = [\"adm0\", \"adm1\", \"adm2\", \"adm3\"]\n",
    "firms_columns = [\"brightness\", \"frp\", \"scan\", \"track\", \"confidence\", \"type\", \"instrument\", \"datetime\", \"year\"]\n",
    "\n",
    "sdf_firm_adm = Adapter.toDf(query_result, adm_columns, firms_columns, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(leftgeometry=<shapely.geometry.polygon.Polygon object at 0x7f81fd30b580>, adm0='ARG', adm1='ARG.8_1', adm2='ARG.8.1_1', adm3='null', rightgeometry=<shapely.geometry.point.Point object at 0x7f81fd30b0d0>, brightness='353.5', frp='33.81', scan='0.56', track='0.69', confidence='n', type='0', instrument='VIIRS', datetime='2021-01-08 18:36', year='2021')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_firm_adm.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- leftgeometry: geometry (nullable = true)\n",
      " |-- adm0: string (nullable = true)\n",
      " |-- adm1: string (nullable = true)\n",
      " |-- adm2: string (nullable = true)\n",
      " |-- adm3: string (nullable = true)\n",
      " |-- rightgeometry: geometry (nullable = true)\n",
      " |-- brightness: string (nullable = true)\n",
      " |-- frp: string (nullable = true)\n",
      " |-- scan: string (nullable = true)\n",
      " |-- track: string (nullable = true)\n",
      " |-- confidence: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- instrument: string (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/06 12:50:20 ERROR TaskSchedulerImpl: Lost executor 1 on 10.1.220.149: The executor with id 1 was deleted by a user or the framework.\n",
      "22/03/06 12:50:29 ERROR TaskSchedulerImpl: Lost executor 2 on 10.1.220.144: The executor with id 2 was deleted by a user or the framework.\n"
     ]
    }
   ],
   "source": [
    "sdf_firm_adm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# set dynamic mode to preserve previous month of times saved\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"geometry\", StringType(), False),\n",
    "    StructField(\"brightness\", FloatType(), True),\n",
    "    StructField(\"frp\", FloatType(), True),\n",
    "    StructField(\"scan\", FloatType(), True),\n",
    "    StructField(\"track\", FloatType(), True),\n",
    "    StructField(\"confidence\", StringType(), True),\n",
    "    StructField(\"type\", IntegerType(), True),\n",
    "    StructField(\"instrument\", StringType(), True),\n",
    "    StructField(\"datetime\", TimestampType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"adm0\", StringType(), False),\n",
    "    StructField(\"adm1\", StringType(), False),\n",
    "    StructField(\"adm2\", StringType(), False),\n",
    "    StructField(\"adm3\", StringType(), True)\n",
    "])\n",
    "\n",
    "sdf_firm_adm = sdf_firm_adm.selectExpr(\n",
    "    \"ST_AsText(rightgeometry) AS geometry\",\n",
    "    \"CAST(brightness as FLOAT)\",\n",
    "    \"CAST(frp as FLOAT)\",\n",
    "    \"CAST(scan as FLOAT)\",\n",
    "    \"CAST(track as FLOAT)\",\n",
    "    \"confidence\",\n",
    "    \"CAST(type as INT)\",\n",
    "    \"instrument\",\n",
    "    \"TO_TIMESTAMP(datetime, 'yyyy-MM-dd HH:mm') as datetime\",\n",
    "    \"CAST(year as INT)\",\n",
    "    \"adm0\",\n",
    "    \"adm1\",\n",
    "    \"adm2\",\n",
    "    \"adm3\"\n",
    ").repartition(\"adm0\", \"adm1\", \"year\")\n",
    "\n",
    "sdf_firm_adm.write \\\n",
    "    .partitionBy(\"adm0\", \"adm1\", \"year\") \\\n",
    "    .option(\"schema\",schema) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .save(\"s3a://dutrajardim-fi/tables/firms.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 23:27:36 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (expr, broadcast, substring as s_substring, trim as s_trim)\n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType, FloatType)\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import KryoSerializer, SedonaKryoRegistrator\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf()\n",
    "parser = configparser.ConfigParser()\n",
    "parser.optionxform=str\n",
    "parser.read_file(open('../sparkconf.cfg'))\n",
    "\n",
    "for section, config in parser.items():\n",
    "    for key, value in config.items():\n",
    "        sparkConf.set(key, value)\n",
    "\n",
    "sparkConf.set(\"spark.serializer\", KryoSerializer.getName)\n",
    "sparkConf.set(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "sparkConf.set(\"spark.kryoserializer.buffer.max\", \"512\")\n",
    "\n",
    "sparkConf.set(\"spark.archives\", \"https://minio.minio-tenant/dutrajardim-etls/dependencies.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=admin%2F20220228%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220228T111424Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=a20e1526dd993354ae90c90c7cb76da4726a5ae00c9920ceec8e2a35f3482c3c#deps\")\n",
    "sparkConf.set(\"spark.executorEnv.PYTHONPATH\", \"/opt/spark/work-dir/deps\")\n",
    "sparkConf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", \"/opt/spark/work-dir/deps/Shapely.libs\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c77c8bc99c5ca9a742a1959efc4a0de604832b3555d74770d62233220eb12265"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
