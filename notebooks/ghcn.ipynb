{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (expr, broadcast)\n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType, FloatType, TimestampType)\n",
    "\n",
    "# from sedona.register import SedonaRegistrator\n",
    "# from sedona.utils import (KryoSerializer, SedonaKryoRegistrator)\n",
    "# from sedona.utils.adapter import Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf()\n",
    "parser = configparser.ConfigParser()\n",
    "parser.optionxform=str\n",
    "parser.read_file(open('../sparkconf.cfg'))\n",
    "\n",
    "for section, config in parser.items():\n",
    "    for key, value in config.items():\n",
    "        sparkConf.set(key, value)\n",
    "\n",
    "# sparkConf.set(\"spark.serializer\", KryoSerializer.getName)\n",
    "# sparkConf.set(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "\n",
    "# sparkConf.set(\"spark.archives\", \"https://minio.minio-tenant/dutrajardim-etls/dependencies.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=admin%2F20220228%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220228T111424Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=a20e1526dd993354ae90c90c7cb76da4726a5ae00c9920ceec8e2a35f3482c3c#deps\")\n",
    "# sparkConf.set(\"spark.executorEnv.PYTHONPATH\", \"/opt/spark/work-dir/deps\")\n",
    "# sparkConf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", \"/opt/spark/work-dir/deps/Shapely.libs\")\n",
    "# sparkConf.set(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.38:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://127.0.0.1:16443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Local spark session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f019051ef40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SedonaRegistrator.registerAll(spark)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/06 17:09:03 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "22/03/06 17:09:03 WARN AmazonHttpClient: SSL Certificate checking for endpoints has been explicitly disabled.\n"
     ]
    }
   ],
   "source": [
    "bucket = \"s3a://dutrajardim-fi\"\n",
    "s3_source = \"%s/src/ncdc/ghcn/{2018,2019,2020,2021}.csv.gz\" % bucket\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"station\", StringType(), False),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "    StructField(\"element\", StringType(), False),\n",
    "    StructField(\"value\", StringType(), False),\n",
    "    StructField(\"measurement_flag\", StringType(), True),\n",
    "    StructField(\"quality_flag\", StringType(), True),\n",
    "    StructField(\"source_flag\", StringType(), True),\n",
    "    StructField(\"obs_time\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_ghcn = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(s3_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_stations = spark.read.format(\"parquet\").load(\"s3a://dutrajardim-fi/tables/stations.parquet\")\n",
    "df_ghcn_station = broadcast(df_stations).join(df_ghcn, on=expr(\"id = station\"), how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# set dynamic mode to preserve previous month of times saved\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "df_ghcn_station \\\n",
    "    .selectExpr(\n",
    "        \"station\",\n",
    "        \"CAST (value aS INT) as value\",\n",
    "        \"element\",\n",
    "        \"measurement_flag\",\n",
    "        \"quality_flag\",\n",
    "        \"source_flag\",\n",
    "        \"TO_TIMESTAMP(CONCAT(date, CASE WHEN obs_time IS NULL THEN '0000' ELSE obs_time END), 'yyyyMMddHHmm') as datetime\",\n",
    "        \"adm0\",\n",
    "        \"adm1\",\n",
    "        \"adm2\",\n",
    "        \"adm3\"\n",
    "    ) \\\n",
    "    .withColumn(\"year\", expr(\"YEAR(datetime)\")) \\\n",
    "    .repartition(\"element\", \"adm0\", \"adm1\", \"year\") \\\n",
    "    .write \\\n",
    "    .partitionBy(\"element\", \"adm0\", \"adm1\", \"year\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .save(\"s3a://dutrajardim-fi/tables/ghcn.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/06 17:18:06 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c77c8bc99c5ca9a742a1959efc4a0de604832b3555d74770d62233220eb12265"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
