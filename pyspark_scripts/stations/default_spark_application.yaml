apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
# metadata:
  # name: stations-spark-script
  # namespace: spark-apps
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "dutradocker/spark-py:3.2.0"
  imagePullPolicy: Always
  # mainApplicationFile: s3a://dutrajardim-fi/spark_scripts/stations_spark_etl.py
  sparkVersion: "3.2.0"
  # deps:
    # packages:
    # - org.apache.hadoop:hadoop-aws:3.2.2
    # - org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.1-incubating
    # - org.datasyslab:geotools-wrapper:1.1.0-25.2
    # pyFiles:
    # - https://minio.minio-tenant/dutrajardim-etls/dependencies.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=admin%2F20220320%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220320T201000Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=10d33390dbf4d7a052d7f14e11fe26fa23c6c4ae3cbfa7ac51a89f6dc06ec949
  sparkConf:
    # spark.archive: s3a://dutrajardim-etls/dependencies.zip#deps
    spark.eventLog.enabled: "true"
    spark.eventLog.dir: "s3a://spark-logs/events"
    spark.serializer: "org.apache.spark.serializer.KryoSerializer"
    spark.kryo.registrator: "org.apache.sedona.core.serde.SedonaKryoRegistrator"
    spark.kryoserializer.buffer.max: "512"
    # spark.app.name: "DJ - Station Information"
    spark.jars.ivy: "/tmp/ivy"
    # spark.jars.packages: "org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.1-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2"
    # spark.executorEnv.PYTHONPATH: "/opt/spark/work-dir/deps"
    # spark.executorEnv.LD_LIBRARY_PATH: /opt/spark/work-dir/deps/Shapely.libs
  hadoopConf:
    fs.s3a.aws.credentials.provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    fs.s3a.path.style.access: "true"
    fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    # fs.s3a.endpoint: "https://minio.minio-tenant.svc.cluster.local"
    # fs.s3a.access.key: "admin"
    # fs.s3a.secret.key: "6bd71ace-8866-407a-9bcc-714bc5753f18"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 1
    # onFailureRetryInterval: 10
    onSubmissionFailureRetries: 1
    # onSubmissionFailureRetryInterval: 20
  driver:
    serviceAccount: spark
    cores: 1
    memory: "4g"
    labels:
      version: 3.2.0
  executor:
    serviceAccount: spark
    cores: 1
    instances: 2
    memory: "4g"
    labels:
      version: 3.2.0
